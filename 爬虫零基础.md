## 前言

参考链接 [2020年Python爬虫全套课程（学完可做项目）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Yh411o7Sz?p=57&spm_id_from=pageDriver)

我主要在教程的基础上进行了相应补充，对于一些点进行了补充和删改，尽量让你能够看懂这篇文章

如果还是有不懂的地方或者有错误的地方可以参考B站的教程，链接见上

对你而言，爬虫只是一种工具，因此只需要知道如何利用这个工具获取你所需要的数据即可，对于爬虫的原理你完全可以不用了解，因此这篇文档主要目的就在于能够让你学会如何简单地写一个爬虫程序，如何使用爬虫

> 禁止将爬虫用于商业用途！！！

## 预备知识

### 前置基础

必备：python基础、HTML/CSS基础

可选：网络协议基础（有最好，没有也行），ajax基础

> 通用处理中文乱码的解决方案：
>
> ```python
> 标识符.encode('iso-8859-1').decode('gbk')
> ```

### 名词解释

#### 反爬机制

门户网站可以通过制定相关的策略或者技术手段防止爬虫程序进行网站数据的爬取

#### 反反爬策略

爬虫程序可以通过制定相关的策略或者技术手段破解门户网站汇总具备的反爬机制，从而可以获取网站数据

#### http协议

http协议指超文本传输协议，是服务器和客户端进行数据交互的一种形式

> 学习爬虫时，对于这个协议，只需要记住下方三种头信息即可

http常用请求头信息

- User-Agent：请求载体的身份标识（请求载体指发起请求的人）
- Connection：请求完毕后是断开连接（close）还是保持连接（keep live）

http常用响应头信息

- Content-Type：服务器响应回客户端的数据类型

#### 标签

在html文档中（即网页中），由一尖括号括起来的就是标签，形如<...>....</...>或者<.../>

例如：`<a href="#" class="className">这是一个a标签</a>`

其中被括在<>中的内容被称为属性，例如上面的a标签具有属性href和class，其中href的属性值为#，class的属性值为className

#### 请求

由客户端向服务器发送的内容就叫做请求，请求由请求头，请求体等构成，这个在ajax请求中会再次介绍

#### 响应

服务器根据客户端发送的请求回应给客户端的内容我们称之为响应，响应由响应头，响应体等构成，这个在ajax请求中会再次介绍

### 爬虫三部曲

爬虫的工作基本围绕着以下三步展开，因此我们学习爬虫也是围绕着以下三步进行的

1. 发送请求并获取响应数据
2. 数据解析
3. 持久化存储

## 发送请求

### requests模块

requests模块是python中原生的一款基于网络请求的模块，功能非常强大，简单便捷效率高，作用是模拟浏览器发请求

#### request模块方法介绍

```python
import requsets

# 向浏览器发送get请求，并将返回响应内容
# 参数说明
# url参数	想要发送请求的网页
# params参数	发送的请求中所要携带的数据
# headers参数	发送的请求中请求头所携带的参数
response = requests.get(url='网址',params = param,headers=headers)	
# 向浏览器发送post请求，并将返回响应内容
# 参数说明
# url参数	想要发送请求的网页
# data参数	发送的请求中所要携带的数据
# headers参数	发送的请求中请求头所携带的参数
response = requests.post(url='网址',data = data,headers=headers)	

# 创建session对象
session = requests.Session()

# 因为上述两种请求返回的都是对象形式的数据，因此需要用对应的方法转化其中的数据格式，三种方法见下
response.text	# 获取字符串格式的响应数据
response.json()	# 获取json格式的响应数据
response.content()	# 获取图片形式的响应数据
```

> JSON数据的格式形如：
>
> ```json
> {
> 	'id':'123',
> 	'name':'w'
> }
> ```
>
> 你可以简单地将JSON格式数据与字典类型数据等价，因为这两者的调用方式在python中是相同的

#### UA伪装和UA检测

- UA伪装

指User-Agent伪装，即让爬虫伪装成某一款浏览器，从而使得爬虫不会被服务器识别出来

- 示范样例（爬取搜狗搜索主页）

```python
import requests
# 指定py文件执行入口
if __name__ == "__main__":
    # UA伪装：将某款浏览器对应的User-Agent封装到一个字典中
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36 Edg/99.0.1150.55'
    }
	# 指定url
    url = "https://www.sogou.com/web"
    # 处理url携带的参数：封装到字典中
    kw = input('enter a word:')
    param = {
        'query': kw
    }
    # 使用get方式发起请求，该方法会返回一个响应对象
    response = requests.get(url=url,params = param,headers=headers)
    # 获取响应数据，text返回的是字符串形式的响应数据，此处的为HTML结构
    page_text = response.text
    # 存储数据
    with open('./'+param['query']+'.html','w',encoding='utf-8') as fp:
        fp.write(page_text)
    print('爬取数据结束')
```

#### 获取局部数据（捕获ajax请求）

##### 如何查看ajax请求

1. 在浏览器中点击F12键打开浏览器开发者工具，然后就会出现如下页面

![image-20220405182539467](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405182539467.png)

2. 打开网络选项卡，进入网络选项卡界面

![image-20220405182855771](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405182855771.png)

3. 同时按住ctrl+R并松开，捕获网络请求，之后将出现如下界面

![image-20220405182923322](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405182923322.png)

4. 选择Fetch/XHR（XHR为ajax数据包），其中的请求均为当前网站向服务器发送的ajax请求

> 这个网站目前只发送了一个ajax，但有些网站一打开就已经发送了几十上百个ajax请求

![image-20220405183049285](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405183049285.png)

4. 双击ajax请求打开ajax请求详情界面

![image-20220405183219499](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405183219499.png)

以上就是如何查看ajax请求的方法，因为这个网页的ajax请求有点奇怪，因此我下面使用了百度网站的ajax请求进行后续的讲解，你也可以自己手动尝试一下如何打开百度网站的ajax请求页面

![image-20220405184831184](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405184831184.png)

在百度的ajax请求中，当前展示的是ajax请求的标头，在标头中，需要观察的内容主要有如下几点

1. **请求 URL**  	标明了ajax请求是发送给谁的，注意真正的请求是?号前面的地址，?号和后面的内容自定义参数
2. **请求方法**       表示了ajax请求的请求方法是什么，请求方法有get、post
3. **状态代码**       状态代码即表示这个请求的响应状态码是多少，只需要记住200代表成功即可，其他的状态码均表示当前请求发送了某些问题
4. **Content-Type**  指出了这个请求响应回来的数据格式是什么，text/plain表示返回的是text格式的数据，因此我们可以使用response.text解析它
5. **cookie**          这个稍后会细讲
6. **Referer**        请求表示请求的来源，一些网站利用这个设置了反爬策略
7. **User-Agent** UA伪装所利用的请求内容

此外，在负载选项卡中存储了此次请求所携带的自定义参数

![image-20220405185115092](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405185115092.png)

例如图片中展示的这次请求携带的参数如下：

```JSON
{
	prod: pc_his,
    from: pc_web,
    json: 1,
    sid: '31253_36020_36005_36088_36166_34584_36144_36120_36196_35863_26350_35723_22160_36061'，
    hisdata:'',
    _t: 1649155593735,
    req: 2,
	csor: 0
}
```

> 注意：如果ajax请求中带有空参数，这个空参数也必须被传递，否则有可能造成请求失败

最后还有响应选项卡

![image-20220405185335961](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405185335961.png)

响应选项卡展示了这次请求成功后服务器所返回的响应内容

##### 如何发送ajax请求

看完了上面的内容，相信你对ajax请求有了一定的了解，下面我将以百度的请求为例，介绍如何在python中模拟发送ajax请求

```python
import requests
import json
# 指定py文件执行入口
if __name__ == "__main__":
    # UA伪装：将某款浏览器对应的User-Agent封装到一个字典中
    headers = {
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36 Edg/99.0.1150.55'
    }
	# 指定url，注意观察这个url与上面的url的区别
    url = "https://www.baidu.com/sugrec"
	
	#在param字典中定义自定义参数，即负载选项卡中的内容
    param = {
        prod: pc_his,
        from: pc_web,
        json: 1,
        sid: '31253_36020_36005_36088_36166_34584_36144_36120_36196_35863_26350_35723_22160_36061'，
        hisdata:'',	# 空参数也必须被发送
        _t: 1649155593735,
        req: 2,
        csor: 0
    }
    # 使用get方式发起请求，该方法会返回一个响应对象
    response = requests.get(url=url,params = param,headers=headers)
    # 到这里，我们就模拟发送了一次ajax请求，接下来我们将处理服务器响应回来的数据
    page_json = response.json()
    # 存储数据，json.dump方法是存储json格式的方法，因为存储内容有中文，所以要指定为utf8编码格式进行存储
    fp = open('./baidu.json','w',encoding='utf-8')
    json.dump(page_json,fp=fp,ensure_ascii=False)
    print('爬取数据结束')
    # 到这里，如果程序没报错，你其实已经完成了一次局部数据的爬取操作
```

下面是两个更加具体的示范样例，你可以先将下方的代码复制进py文件中尝试一下，再来研究他们是如何工作的

- 示范样例（直接获取百度翻译单个单词翻译的结果）

```python
import requests
import json
# 指定py文件执行入口
if __name__ == "__main__":
    # UA伪装：将某款浏览器对应的User-Agent封装到一个字典中
    headers = {
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36 Edg/99.0.1150.55'
    }
	# 指定百度翻译的url
    url = "https://fanyi.baidu.com/sug"
    # 处理url携带的参数：封装到字典中
	trans = input('输入你要翻译的句子')
    data = {
        'kw': trans	# 这个参数是负载选项卡中可以看见的指定的键值
    }
    # 使用post方式发起请求，该方法会返回一个响应对象
    response = requests.post(url=url,data = data,headers=headers)
    # 获取响应数据，因为服务器返回的是json格式的数据，因此需要用json方法进行解析，注意，必须在确定响应的是json类型才能使用该方法，否则会报错
    page_json = response.json()
    # 存储数据，json.dump方法是存储json格式的方法，因为存储内容有中文，所以要关闭ascii码存储
    fp = open('./trans.json','w',encoding='utf-8')
    json.dump(page_json,fp=fp,ensure_ascii=False)
    print('爬取数据结束')
```

- 示范样例2（爬取豆瓣电影）

```python
import requests
import json
# 指定py文件执行入口
if __name__ == "__main__":
    # UA伪装：将某款浏览器对应的User-Agent封装到一个字典中
    headers = {
        'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36 Edg/99.0.1150.55'
    }
	# 指定url
    url = "https://movie.douban.com/j/chart/top_list"
    # 处理url携带的参数：封装到字典中
    # 有些参数用英文翻译一下其实你应该就能大概猜到这个参数是用来干什么的了，不信你可以修改一下start或者limit参数的值试一试
    param = {
        'type': 24,
		'interval_id': '100:90',
		'start': 0,
		'limit': 20
    }
    # 使用post方式发起请求，该方法会返回一个响应对象
    response = requests.get(url=url,params=param,headers=headers)
    # 获取响应数据，因为服务器返回的是json格式的数据，因此需要用json方法进行解析，注意，必须在确定响应的是json类型才能使用该方法，否则会报错
    page_json = response.json()
    # 存储数据，json.dump方法是存储json格式的方法，因为存储内容有中文，所以要关闭ascii码存储
    fp = open('./film.json','w',encoding='utf-8')
    json.dump(page_json,fp=fp,ensure_ascii=False)
    print('爬取数据结束')
```

#### cookie与session

- cookie

cookie：cookie是在登录成功后服务端发送给客户端用于识别身份的信息，用于记录客户端的登录状态，因此我们可以在发送请求时携带cookie数据，从而使得服务器返回我们想要的结果

> cookie不是唯一一种用于验证登录的信息，还有例如JWT机制等其他方法可以用来识别客户端信息，当前所讲述方法只适用于采用了cookie验证登录的网站

> cookie一般被存放于session对象中，因此可以使用session对象进行模拟登陆post请求的发送（cookie）就会被服务器自动保存于session对象中，之后就可以利用该session对象对服务器发起请求

- 简单示例（下方示例采用了bs4写法且没有添加验证码识别部分，你可以只看发送请求的部分，即step1/4/5）

```python
from lxml import etree
import requests
from bs4 import BeautifulSoup

# step1，预备数据处理
session = requests.Session()	# 创建session对象
headers = {
	'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36 Edg/99.0.1150.55'
}
page_url = '需要登录的网页URL'
login_url = '需要发送登录请求的ajax地址'
img_src = ''	# 用于保存获取到的图片地址
detail_url = '登录后想要爬取的请求信息'

# step2，获取网站验证码图片并保存
page_text = requests.get(url = page_url, headers = headers).text # 获取网站源码
soup = BeautifulSoup(page_text)		# 将获取到的网页源码转化为BS对象
# 保存验证码的地址，注意：验证码一般唯一，且采用了特定标签属性进行记录，如采用alt、id等
img_src = soup.find(img,alt_="alt值")	
# 保存请求的图片二进制数据
img_content = requests.get(url = img_src, headers = headers).content()
# 将保存的图片二进制数据写入图片文件中
with open('./check.jpg','wb') as fp:
	fp.writr(img_content)
    
# step3，解析图片中的验证码，此处getCodeText为封装好的云打码平台验证码验证函数
result = getCodeText('./check.jpg',验证码类型代码)	

#step4，向网站发送登录请求
# 需要用于post请求的参数
data = {
    'icode': result,	# 验证码识别后的值
    ...		# 其余参数需要具体参考对应的ajax请求格式
}
# 使用session对象发送post请求，使得服务器传递的cookie能够被保存
response = session.post(url=login_url, headers=headers, data=data)
print(response.status_code)	# 打印返回的状态码以查看登录是否成功，如果是200则表示请求成功

# step5，爬取需要获取的登录后信息
# 发送请求，下方的post也可以是get，解析数据的方式也可以是json()或者content()，具体视情况而定
detail_page_text = session.post(url=detail_url, headers=headers).text
with open('保存的路径/文件名.文件后缀','w',encoding='utf-t') as fp:
    fp.write(detail_page_text)
```

## 数据解析

讲完了请求发送，下面我们来讲一讲数据解析

数据解析就是利用工具从服务器响应回来的数据中解析出我们想要的数据

数据解析方法一共有三种：

1. 正则表达式
2. bs4模块
3. xpath

其中正则表达式入门较难，bs4模块局限性较强（只能用于python）且需要了解较多的html/css知识，因此不是很推荐。所以在这里我只介绍xpath，如果你想要学习另外两种解析方法的话可以找我要另外一份文档

> 通常情况下正则是必备的，因为正则灵活性较强，但是入门阶段只掌握xpath就能解决80%的问题

### xpath（推荐使用，最常用，通用性最强）

xpath是一门在XML文档中查找信息的语言，python中内置了这门语言

xpath利用了html文档树结构进行查找

#### 使用开发者工具辅助查找xpath路径

使用浏览器打开所要爬取的标签后，点击F12打开开发者工具，使用左上角的标签定位工具定位到所要的标签后，点击右键，会发现复制选项有一个展开选项，里面可以直接复制完整的xpath路径

在利用上述工具的情况下，xpath上手最快，否则xpath入门也不易

![image-20220405190522332](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405190522332.png)

![image-20220405190724412](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220405190724412.png)

以上就是利用开发者工具选择xpath路径的全部过程了

因为网页的结构都是有一定规律的，因此在数据解析前需要进行一个分析网页的过程，而这个过程你可以复制多个你所需要爬取的内容的xpath路径来观察他们的规律，从而找到一个合适的数据解析方式

下面我将正式开始介绍如何使用xpath

#### xpath使用步骤

- 环境安装

```python
pip install lxml	# 只需要这一个包，该包中有etree类
```

- 对象实例化

将html数据加载到etree对象

```python
from lxml import etree

# 加载本地html文档
fp = open('./test.html','r',encoding = 'utf-8')
tree = etree.parse(fp)	# 解析文档字符串

# 加载网页源码
page_text = requests(...).text	# 获取网页源码并返回
tree = etree.HTML(page_text)

# 以上两种对象实例化方法二选一即可

# 调用etree对象的xpath方法
tree.xpath('xpath表达式')
```

#### xpath表达式

```python
# 标签定位，下方标签定位方式可结合使用
/	# 一个斜杠表示单个层级，可以用 /html 指定根节点
//	# 两个斜杠表示多个层级，如tree.xpath('//div')表示查找文档中任意位置的div标签
标签名[@属性名="属性值"]		# 属性定位，定位到具有指定属性的标签
标签名[num]	# 索引定位，例如p[1]表示定位到第一个符合查找条件的p标签，注意索引是从1开始的

# 获取文本内容
标签名/text()	# 获取标签的直系文本内容，不包含子代的文本内容
标签名//text()	# 获取标签的所有文本内容，包含子代的文本内容

# 获取属性内容
标签名/@属性名	# 获取标签的指定属性的属性值
```

> 如果没有指定根节点，则将从文档中任意节点开始查找，返回所有符合的结果
>
> 上面的式子中获取文本内容和属性内容的语法必须要背下来

#### 调用etree对象的xpath方法

```python
tree.xpath('/html/body/div')	# 左边为html文档树写法，返回的是元素对象列表
```

## 持久化存储

所谓持久化存储，其实就是保存数据到文件中，在之前的过程中其实我已经演示过了

持久化存储一般有两种方式，一种是本地存储，一种是数据库存储。

因为数据库存储你目前也不太可能遇见，而且数据库存储还涉及到其他的模块，因此我在这里只介绍本地存储

#### 本地存储

本地存储其实就是利用python的文件操作进行存储

```python
# 方法1
with open('文件路径','文件的打开方式',encoding='编码格式') as fp:
    fp.方法
    
# 方法2
fp = open('文件路径','文件的打开方式',encoding='编码格式')
fp.方法
```

> 文件的打开方式有：
>
> - r	只读方式打开
> - w   写入方式打开
> - rb  只读方式读取二进制文件
> - wb 写入二进制数据
> - a    追加方式写入文件（这个我有点记不清了，可能需要查一下）
>
> 编码格式有：
>
> - utf-8
> - gbk
> - ......
>
> 编码格式一般使用utf-8格式，如果有乱码可以改成gbk格式，如果还是不行，你输入这篇文档最开始的那行代码应该就行了
>
> 文件的方法有：
>
> - write()	写入数据
> - read()     读取数据，返回整篇文档的字符串
> - readlines() 读取一行数据
>
> 注意：python可以自动创建文件但无法自动创建文件夹，因此如果文件夹不存在就会直接报错，所以在存储数据之前要先保证文件夹存在

看完了上面的内容，并且自己手动写几个爬虫后，你的爬虫也基本就入门了，但是记住，这只是入门，接下来我将介绍一个很常用很常用的模块——selenium模块，配合着这个模块，你基本就可以爬取60%的网站的数据了

## selenium模块

### 什么是selenium模块

selenium模块是实现浏览器自动化操作的一个模块，你可以简单理解为使用了这个模块，就相当于有一个机器人在自动帮助你操控你的浏览器，让你无需手动操作

我之所以在爬虫三部曲后要着重介绍这个模块是因为selenium模块是一个很好用很好用的工具，使用这个工具可以破解多数的反爬策略，因此必须要熟练掌握它

### 环境安装

```python
# step1，下载selenium模块，注意，下载3.141版本就好，尽量不要下载最新版，最新版的selenium做了一些改动
pip install selenium==3.141
# step2，主要驱动程序要和你电脑的谷歌版本一一对应
下载某一个浏览器的驱动程序（例如chromedriver）
```

### 使用示例

```python
from selenium import webdriver
bro = webdriver.Chrome(executable_path='浏览器驱动程序目录')	# 这里
bro.get('url地址')	# 发送get请求并将返回的源码直接保存到bro对象中，模拟用户在浏览器地址栏中输入url地址的行为
...	# 执行相关行为
```

> 你可以选择一个url地址执行上面的代码，看看你的电脑会出现上面变化

### selenium常用方法

| 方法名                                          | 描述                                                         |
| ----------------------------------------------- | ------------------------------------------------------------ |
| 标签定位系列.find_...，见下                     |                                                              |
| `bro.find_element_by_id('ID属性值')`            | 通过标签id查找标签，返回所查找的标签                         |
| `bro.find_element_by_css_selector('CSS选择器')` | 通过CSS选择器查找标签，返回所查找的标签                      |
| ...                                             | 其他常用的标签定位方法与js中类似，因此不一一解释             |
| 标签交互系列见下                                |                                                              |
| `定位标签.send_keys('要输入的值')`              | 模拟浏览器输入功能，在所定位的标签中进行输入                 |
| `定位标签.click()`                              | 模拟浏览器点击功能，点击所选择的标签                         |
| `定位标签.location`                             | 这是一个属性，以对象形式返回标签的左上角坐标，可以利用location['x']获取x坐标 |
| `定位标签.size`                                 | 这是一个属性，以对象形式返回标签的大小，可以利用size['width']获取宽度 |
| js代码执行                                      |                                                              |
| `bro.execute_script('js代码')`                  | 在浏览器当前页面执行js代码                                   |
| 前进回退                                        |                                                              |
| `bro.back()`                                    | 浏览器页面回退                                               |
| `bro.forward()`                                 | 浏览器页面前进                                               |
| 截图                                            |                                                              |
| `bro.save_screenshot('图片名')`                 | 截图，传入的字符串参数为截图的保存位置和图片名               |
| 关闭浏览器                                      |                                                              |
| `bro.quit()`                                    | 关闭浏览器，一般要配合sleep()使用，否则可能会出现数据未抓取完毕浏览器就已经关闭的情况 |

> 上面的方法可以不用背，要用的时候查一下就行了
>
> 更多的模块方法请参考官方文档

### selenium与iframe

如果定位的标签是存在于iframe标签中，则必须通过如下操作进行标签定位

```python
bro.switvh_to.iframe('iframeResult')	# 切换浏览器标签定位的作用域
...	# 之后按照之前的方法进行操作即可
```

> 原理：原本的作用域是在外部的iframe中，而该操作是切换作用域操作

### selenium与动作链

- 步骤

1. 导入包
2. 实例化动作链对象
3. 进行动作链操作

- 动作链实例

```python
from selenium import webdriver
from selenium.webdriver import ActionChains

bro = webdriver,Chrome(executable_path='')
bro.get()

# 实例化动作链对象
action = ActionChains(bro)

# 点击并长按指定的标签
action.click_and_hold(标签定位语句)

# 移动对应的偏移量
action.move_by_offset(x坐标偏移量, y坐标偏移量)

# 执行动作链操作
action.perform()

# 释放动作链，此处表现为松开标签
action.release()
```

### 无头浏览器和规避检测

无头浏览器是一个术语，即代表无可视化界面的浏览器，在此处具体操作为关闭selenium操作过程时候的可视化界面

除了使用谷歌外，还可以使用phantomJs，但是phantomJs已经停止维护了，不建议使用

> 由于无头+selenium是一种很常见的爬虫手段，所以很多门户网站都针对此设定了反爬机制，因此我们需要设立反反爬策略进行规避

无头和规避检测代码示例如下，使用时直接将下方代码粘贴于爬虫代码前方即可

```python
from selenium import webdriver
# 实现无可视化界面
from selenium.webdriver.chrome.options import Options
# 实现规避检测
from selenium.webdriver import ChromeOptions

# 实现无头
chrome_option = Options()
chrome_option.add_argument('--headless')
chrome_option.add_argument('--disable-gpu')

# 实现规避检测
option = ChromeOptions()
option.add_experimental_option('excludeSwitches',['enable-automation'])

bro = webdriver.Chrome(executable_path='浏览器驱动安装地址', chrome_options=chrome_option, options=option)

bro.get('网页URL地址')
```

> 利用selenium模块模拟登陆时，需要对selenium打开的页面的验证码进行截图，否则如果直接对图片的src进行请求会出现当前登录请求与验证码不匹配

### 补充知识-图片裁剪

- 所需库

```python
from PIL import Image
```

- 使用方法

```python
from PIL import Image

rangle = (int('要裁剪的图片的左上角x坐标'),int('要裁剪的图片的左上角y坐标'),int('要裁剪的图片的右下角x坐标'),int('要裁剪的图片的右下角y坐标'))
i = Image.open('图片文件路径')
code_img_name = './code.png'	# 图片保存地址
frame = i.crop(rangle)
frame.save(code_img_name)
```

## 进阶模块

总算写到这里了

看完上面的内容，你的爬虫基本就入门了，可以简单爬取一些较为基础的网站，如果你熟悉了selenium模块后，基本可以爬取60%左右的网站。

关于进阶模块，这个模块我没有详细写，因为我也不是很经常用到爬虫，对这些东西并不是特别熟悉，因此我仅仅介绍了进阶模块都有些啥如果你想要了解的话可以找我要完整的学习文档，但是看那篇文档前需要很多前置知识，你如果想要的话就跟我说吧

### 反反爬策略

反反爬策略是用于破解网站反爬策略的手段或者方法，主要有UA伪装、验证码识别、代理IP等等，这属于进阶的内容，需要自行了解

### 异步爬虫

之前我们所讲到的爬虫都是同步（单线程）爬虫，而异步爬虫可以实现多线程爬取数据，使得爬虫爬取的效率更高，但这部分知识需要了解大量的异步编程的知识

### scrapy框架

scrapy框架是目前最流行的爬虫框架，使用该框架可以让爬虫写起来更块，但相应的，上手难度也不低，如果你熟练掌握了前面的内容，可以尝试接触这个模块

### 分布式爬虫

可以利用多台机器爬取同一个网站的技术，可以大大提高爬取效率

## 写在最后

看完了这份文档，恭喜你，爬虫已经入门了，现在相信你如果看一些进阶的资料虽然还是有点艰难，但是相信已经可以略微地看懂他们了

爬虫目前已经不仅仅是一种技术了，而已经作为一种职业出现了，各种反爬和反反爬的策略也层出不穷。因此在爬取东西的时候没有得到想要的结果是很正常的，面对这种情况，要学会善于搜索或者及时提问。

最后，希望这份文件对你今后的升学之路有帮助

<div style="float:right">
    2022年4月5日
</div>

​    

